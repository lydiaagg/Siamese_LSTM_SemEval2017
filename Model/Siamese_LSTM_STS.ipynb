{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese_LSTM_STS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyCAqBpNCnPq"
      },
      "source": [
        "%%capture\n",
        "# Install the latest Tensorflow version.\n",
        "!pip install tensorflow_text\n",
        "!pip install bokeh\n",
        "!pip install simpleneighbors[annoy]\n",
        "!pip install tqdm\n",
        "!pip install unidecode\n",
        "!pip install keras==2.4.3"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpPTBJRYdRI-"
      },
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.layers.merge import multiply, concatenate\n",
        "import itertools\n",
        "import datetime\n",
        "import gensim\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.models  import Model,Sequential\n",
        "from tensorflow.python.keras.layers import Input, Embedding, LSTM, Lambda,Dense,merge\n",
        "import keras.backend as K\n",
        "from keras import layers,regularizers\n",
        "from keras.optimizers import Adadelta, SGD, RMSprop, Adagrad, Adam, Adamax, Nadam\n",
        "from keras.layers import Input,Embedding, LSTM, Dense, Flatten, Activation, RepeatVector, Permute, Lambda, \\\n",
        "    Bidirectional, TimeDistributed, Dropout, Conv1D, GlobalMaxPool1D\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "mtWEbDfhE9E-",
        "outputId": "48b8832c-0f9b-46c9-b908-4e2cb62493c1"
      },
      "source": [
        "!pip install patool\n",
        "import patoolib\n",
        "patoolib.extract_archive(\"/content/drive/MyDrive/ar_wiki_word2vec-300.rar\", outdir=\"/content/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 2.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n",
            "patool: Extracting /content/drive/MyDrive/ar_wiki_word2vec-300.rar ...\n",
            "patool: running /usr/bin/unrar x -- /content/drive/MyDrive/ar_wiki_word2vec-300.rar\n",
            "patool:     with cwd='/content/'\n",
            "patool: ... /content/drive/MyDrive/ar_wiki_word2vec-300.rar extracted to `/content/'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sJF1UJaMdggd",
        "outputId": "6ac3c861-ee15-4e68-da4e-bf2476aee064"
      },
      "source": [
        "# File paths\n",
        "TRAIN_CSV = '/content/ar-en-train.csv'\n",
        "TEST_CSV = '/content/ar-en.csv'\n",
        "EMBEDDING_FILE = '/content/drive/MyDrive/RandomShuffle_Skip/randshuffle_5window_skipgram_300size.model'\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "train_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>english</th>\n",
              "      <th>arabic</th>\n",
              "      <th>sts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Someone carries a skateboard night on the side...</td>\n",
              "      <td>رجل جالس بمفرده يقرأ على طاولة مستديرة ، خارج ...</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>The women race in the Daytona 500.\\n</td>\n",
              "      <td>يتسابق بعض الرجال ضمن مسابقة التزلج</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Women walk side by side.\\n</td>\n",
              "      <td>هناك فتيات يمشين متجاورات</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>The guy with the green shirt jumps up on the g...</td>\n",
              "      <td>يمشي الرجل ذو القميص الأبيض على العشب الطويل م...</td>\n",
              "      <td>2.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Two men sitting on the lawn with bananas.\\n</td>\n",
              "      <td>ثلاثة رجال يتسكعون عند فرشة بيع الفاكهة</td>\n",
              "      <td>1.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...  sts\n",
              "0           0  ...  0.8\n",
              "1           1  ...  1.0\n",
              "2           2  ...  2.6\n",
              "3           3  ...  2.2\n",
              "4           4  ...  1.4\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWxP8CWiPQXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9937e0ee-481b-410b-f035-dbb03c71217c"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "ar_stopwords = '''\n",
        "أنفسنا مثل حيث ذلك بشكل لدى ألا عن إلي ب لنا وقالت فقط الذي الذى ا هذا غير أكثر اي أنا أنت ايضا اذا كيف وكل أو اكثر أي أن منه وكان وفي تلك إن سوف حين نفسها هكذا قبل حول منذ هنا عندما على ضمن لكن فيه عليه قليل صباحا لهم بان يكون بأن أما هناك مع فوق بسبب ما لا هذه و فيها ف ولم ل آخر ثانية انه من الان جدا به بن بعض حاليا بها هم أ كانت هي لها نحن تم أنفسهم ينبغي إلى فان وقد تحت' عند وجود الى فأن الي او قد خارج إنه اى مرة هؤلاء أنها إذا فهي فهى كل يمكن جميع أنفسكم فعل كان ثم لي الآن وقال فى في ديك لم لن له تكون الذين ليس التى التي أنه وان بعد حتى ان دون وأن لماذا يجري كلا إنها لك ضد وإن فهو انها منها أى لديه ولا بين خلال وما اما عليها بعيدا كما نفسي نحو هو نفسك نفسه انت ولن إضافي لقاء وكانت هى فما أيضا إلا معظم ومن إما الا بينما وهي وهو وهى\n",
        "'''\n",
        "\n",
        "ar_stopwords=nltk.word_tokenize(ar_stopwords)\n",
        "print(\"length of stopwords is: \",len(ar_stopwords))\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "length of stopwords is:  179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd72zru33woU"
      },
      "source": [
        "def normalize(df, feature_names):\n",
        "    result = df.copy()\n",
        "    for feature_name in feature_names:\n",
        "        max_value = df[feature_name].max()\n",
        "        min_value = df[feature_name].min()\n",
        "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
        "    return result"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKV8bZs6dsGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd28fcd0-abd1-463b-c61c-c7309bfd041a"
      },
      "source": [
        "# Load training and test set\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "#normalisation\n",
        "\"\"\"\n",
        "train_df =normalize(train_df, [\"sts\"])\n",
        "test_df =normalize(test_df, [\"sts\"])\n",
        "\"\"\"\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "def text_to_word_list(text):\n",
        "    ''' Pre process and convert texts to a list of words '''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    return text\n",
        "def text_to_arabic_word_list(text):\n",
        "    search = [\"أ\", \"إ\", \"آ\", \"ة\", \"_\", \"-\", \"/\", \".\", \"،\", \" و \", \" يا \", '\"', \"ـ\", \"'\", \"ى\", \"\\\\\", '\\n', '\\t',\n",
        "              '&quot;', '?', '؟', '!']\n",
        "    replace = [\"ا\", \"ا\", \"ا\", \"ه\", \" \", \" \", \"\", \"\", \"\", \" و\", \" يا\", \"\", \"\", \"\", \"ي\", \"\", ' ', ' ', ' ', ' ? ', ' ؟ ',\n",
        "               ' ! ']\n",
        "    text = str(text)\n",
        "\n",
        "    # remove tashkeel\n",
        "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    text = re.sub(p_tashkeel, \"\", text)\n",
        "\n",
        "    # remove longation\n",
        "    p_longation = re.compile(r'(.)\\1+')\n",
        "    subst = r\"\\1\\1\"\n",
        "    text = re.sub(p_longation, subst, text)\n",
        "\n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "\n",
        "    for i in range(0, len(search)):\n",
        "        text = text.replace(search[i], replace[i])\n",
        "\n",
        "    # trim & split\n",
        "    text = text.strip()\n",
        "    text = text.split()\n",
        "    \n",
        "    return text\n",
        "# Prepare embedding\n",
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
        "word2vec3 = Word2Vec.load('/content/ar_wiki_word2vec')\n",
        "word2vec = Word2Vec.load(EMBEDDING_FILE)\n",
        "\n",
        "sent_cols = ['english', 'arabic']\n",
        "i=0\n",
        "# Iterate over the sentences only of both training and test datasets\n",
        "for dataset in [train_df, test_df]:\n",
        "    for index, row in dataset.iterrows():\n",
        "\n",
        "        # Iterate through the text of both sentences of the row\n",
        "        for sent in sent_cols:\n",
        "            q2n = []  # q2n -> sentence numbers representation\n",
        "            if sent =='english':\n",
        "               sentence=text_to_word_list(row[sent])\n",
        "            else:\n",
        "               sentence=text_to_arabic_word_list(row[sent])\n",
        "            for word in sentence:\n",
        "              \n",
        "                # Check for unwanted words\n",
        "                if word in stops :\n",
        "                    continue\n",
        "                if word in ar_stopwords:\n",
        "                    continue\n",
        "                if word not in word2vec.wv.vocab :\n",
        "                   if word not in word2vec3.wv.vocab :\n",
        "                     i=i+1\n",
        "                     continue\n",
        "              \n",
        "                if word not in vocabulary:\n",
        "                    vocabulary[word] = len(inverse_vocabulary)\n",
        "                    q2n.append(len(inverse_vocabulary))\n",
        "                    inverse_vocabulary.append(word)\n",
        "                else:\n",
        "                    q2n.append(vocabulary[word])\n",
        "\n",
        "            # Replace sentences as word to sentences as number representation\n",
        "            dataset.at[index, sent]= q2n\n",
        "print(i)           \n",
        "embedding_dim = 300\n",
        "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "embeddings[0] = 0  # So that the padding will be ignored\n",
        "# Build the embedding matrix\n",
        "for word, index in vocabulary.items():\n",
        "    if word in word2vec.wv.vocab:\n",
        "        embeddings[index] = word2vec.wv[word]\n",
        "        \n",
        "    elif word in word2vec3.wv.vocab:\n",
        "        embeddings[index] = word2vec3.wv[word]\n",
        "        \n",
        "        \n",
        "del word2vec"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbTnYvh3eGCf"
      },
      "source": [
        "max_seq_length = max(train_df.english.map(lambda x: len(x)).max(),\n",
        "                     train_df.arabic.map(lambda x: len(x)).max(),\n",
        "                     test_df.english.map(lambda x: len(x)).max(),\n",
        "                     test_df.arabic.map(lambda x: len(x)).max())\n",
        "\n",
        "# Split to train validation\n",
        "validation_size = int(len(train_df) * 0.1)\n",
        "training_size = len(train_df) - validation_size\n",
        "\n",
        "X = train_df[sent_cols]\n",
        "Y = train_df['sts']\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
        "\n",
        "# Split to dicts\n",
        "X_train = {'left': X_train.english, 'right': X_train.arabic}\n",
        "X_validation = {'left': X_validation.english, 'right': X_validation.arabic}\n",
        "X_test = {'left': test_df.english, 'right': test_df.arabic}\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values\n",
        "\n",
        "# Zero padding\n",
        "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train['left'].shape == X_train['right'].shape\n",
        "assert len(X_train['left']) == len(Y_train)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPxRlsL4X5Tc"
      },
      "source": [
        "def cosine_distance(vecs):\n",
        "    \n",
        "    y_true, y_pred = vecs\n",
        "    y_true = K.l2_normalize(y_true, axis=-1)\n",
        "    y_pred = K.l2_normalize(y_pred, axis=-1)\n",
        "    return K.mean(1 - K.sum((y_true * y_pred), axis=-1))\n",
        "\n",
        "def cosine_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    print((shape1[0], 1))\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "    \n",
        "def exponent_neg_euclidean_distance(left, right):\n",
        "    \"\"\" Helper function for the similarity estimate of the LSTMs outputs\"\"\"\n",
        "    return K.sqrt(K.sum(K.square(left - right), axis=-1, keepdims=True))\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    return K.mean((1 - y_true) * K.square(y_pred) +  y_true* K.square(K.maximum(margin - y_pred, 0)),axis=-1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLZApEDieK0i",
        "outputId": "0254b980-750a-470e-94ce-d7dc54c1f3d2"
      },
      "source": [
        "# Model variables\n",
        "\n",
        "gradient_clipping_norm = 0.1\n",
        "batch_size =1024\n",
        "n_epoch =64\n",
        "\n",
        "\n",
        "# The visible layer\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "# Embedded version of the inputs\n",
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "# Since this is a siamese network, both sides share the same LSTM\n",
        "shared_lstm=Sequential()\n",
        "shared_lstm.add(LSTM(256,return_sequences=True))\n",
        "shared_lstm.add(Dense(256, activation='relu'))  \n",
        "\n",
        "left_output = shared_lstm(encoded_left)\n",
        "right_output = shared_lstm(encoded_right)\n",
        "\n",
        "# Calculates the distance as defined by the MaLSTM model\n",
        "#malstm_distance = Lambda(function=lambda x: exponent_neg_euclidean_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "malstm_distance= Lambda(cosine_distance, output_shape=cosine_dist_output_shape)([left_output, right_output])\n",
        "\n",
        "malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "optimizer = Adam(clipnorm=gradient_clipping_norm)\n",
        "malstm.compile(loss='mse', optimizer=optimizer)\n",
        "\n",
        "# Start training\n",
        "training_start_time = time()\n",
        "malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size,epochs=n_epoch,\n",
        "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
        "malstm.save('SiameseLSTM.h5')\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/64\n",
            "1/1 [==============================] - 6s 6s/step - loss: 3.7833 - val_loss: 7.0004\n",
            "Epoch 2/64\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 5.7139 - val_loss: 6.8589\n",
            "Epoch 3/64\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 5.5789 - val_loss: 6.7656\n",
            "Epoch 4/64\n",
            "1/1 [==============================] - 1s 562ms/step - loss: 5.4947 - val_loss: 6.6839\n",
            "Epoch 5/64\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 5.4318 - val_loss: 6.6233\n",
            "Epoch 6/64\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 5.3944 - val_loss: 6.5976\n",
            "Epoch 7/64\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 5.3759 - val_loss: 6.5853\n",
            "Epoch 8/64\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 5.3683 - val_loss: 6.5755\n",
            "Epoch 9/64\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 5.3653 - val_loss: 6.5696\n",
            "Epoch 10/64\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 5.3647 - val_loss: 6.5545\n",
            "Epoch 11/64\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 5.3644 - val_loss: 6.5541\n",
            "Epoch 12/64\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 5.3642 - val_loss: 6.5540\n",
            "Epoch 13/64\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 5.3641 - val_loss: 6.5538\n",
            "Epoch 14/64\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 5.3641 - val_loss: 6.5535\n",
            "Epoch 15/64\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 5.3641 - val_loss: 6.5532\n",
            "Epoch 16/64\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 5.3641 - val_loss: 6.5529\n",
            "Epoch 17/64\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 5.3641 - val_loss: 6.5524\n",
            "Epoch 18/64\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 5.3641 - val_loss: 6.5520\n",
            "Epoch 19/64\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 5.3641 - val_loss: 6.5515\n",
            "Epoch 20/64\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 5.3641 - val_loss: 6.5514\n",
            "Epoch 21/64\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 22/64\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 23/64\n",
            "1/1 [==============================] - 1s 575ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 24/64\n",
            "1/1 [==============================] - 1s 559ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 25/64\n",
            "1/1 [==============================] - 1s 555ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 26/64\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 27/64\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 28/64\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 29/64\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 30/64\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 31/64\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 32/64\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 33/64\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 34/64\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 35/64\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 36/64\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 37/64\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 38/64\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 39/64\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 40/64\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 41/64\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 42/64\n",
            "1/1 [==============================] - 1s 569ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 43/64\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 44/64\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 45/64\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 46/64\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 47/64\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 48/64\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 49/64\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 50/64\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 51/64\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 52/64\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 53/64\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 54/64\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 55/64\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 56/64\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 57/64\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 58/64\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 59/64\n",
            "1/1 [==============================] - 1s 557ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 60/64\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 61/64\n",
            "1/1 [==============================] - 1s 566ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 62/64\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 63/64\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Epoch 64/64\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 5.3641 - val_loss: 6.5513\n",
            "Training time finished.\n",
            "64 epochs in 0:00:45.596505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWAfbSETiv7D",
        "outputId": "702da329-6974-4bd8-d103-79d7bc047296"
      },
      "source": [
        "for dataset, side in itertools.product([X_test], ['left', 'right']):\n",
        "        dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
        "\n",
        "sims = malstm.predict([X_test['left'], X_test['right']],1)\n",
        "print(sims)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.21428575 0.5        0.42857146 0.42857146 0.5714286  0.5714286\n",
            " 0.5        0.28571433 0.21428575 0.14285718 0.21428575 0.28571433\n",
            " 0.3571429  0.42857146 0.3571429  0.28571433 0.5714286  0.3571429\n",
            " 0.28571433 0.71428573 0.85714287 0.28571433 0.3571429  0.64285713\n",
            " 0.42857146 0.28571433 0.14285718 0.42857146 0.42857146 0.28571433\n",
            " 0.28571433 0.28571433 0.28571433 0.5714286  0.14285718 0.28571433\n",
            " 0.21428575 0.14285718 0.3571429  0.64285713 0.5        0.5714286\n",
            " 0.42857146 0.21428575 0.14285718 0.28571433 0.21428575 0.71428573\n",
            " 1.         0.5        0.21428575 0.21428575 0.28571433 0.3571429\n",
            " 0.28571433 0.42857146 0.42857146 0.71428573 0.3571429  0.3571429\n",
            " 0.64285713 0.21428575 0.5        0.21428575 0.5        0.28571433\n",
            " 0.28571433 0.42857146 0.42857146 0.3571429  0.21428575 0.42857146\n",
            " 0.28571433 0.3571429  0.28571433 0.28571433 0.28571433 0.21428575\n",
            " 0.3571429  0.78571427 0.3571429  0.5        0.28571433 0.5714286\n",
            " 0.3571429  0.21428575 0.28571433 0.3571429  0.21428575 0.5714286\n",
            " 0.28571433 0.3571429  0.3571429  0.3571429  0.5714286  0.3571429\n",
            " 0.3571429  0.28571433 0.28571433 0.21428575 0.28571433 0.14285718\n",
            " 0.3571429  0.5714286  0.64285713 0.14285718 0.21428575 0.3571429\n",
            " 0.42857146 0.28571433 0.5714286  0.42857146 0.3571429  0.42857146\n",
            " 0.28571433 0.85714287 0.3571429  0.3571429  1.         0.5\n",
            " 0.42857146 0.64285713 0.28571433 0.3571429  0.28571433 0.21428575\n",
            " 0.42857146 0.14285718 0.5714286  0.3571429  0.42857146 0.42857146\n",
            " 0.3571429  0.21428575 0.3571429  0.21428575 0.3571429  0.71428573\n",
            " 0.14285718 0.64285713 0.3571429  0.3571429  0.42857146 0.28571433\n",
            " 0.3571429  0.3571429  0.28571433 0.5        0.21428575 0.21428575\n",
            " 0.21428575 0.5714286  0.5714286  0.5        0.28571433 0.42857146\n",
            " 0.3571429  0.64285713 0.28571433 0.28571433 0.3571429  0.64285713\n",
            " 0.5714286  0.5714286  0.3571429  0.64285713 0.28571433 0.3571429\n",
            " 0.28571433 0.5714286  0.21428575 0.28571433 0.64285713 0.42857146\n",
            " 0.21428575 0.71428573 0.3571429  0.21428575 0.28571433 0.3571429\n",
            " 0.42857146 0.42857146 0.28571433 0.5        0.21428575 0.21428575\n",
            " 0.14285718 0.21428575 0.5714286  0.42857146 0.5        0.5\n",
            " 0.21428575 1.         0.5714286  0.3571429  0.21428575 0.42857146\n",
            " 0.5714286  0.28571433 0.3571429  0.42857146 0.42857146 0.21428575\n",
            " 0.85714287 0.28571433 0.5        0.28571433 0.21428575 0.42857146\n",
            " 0.28571433 0.28571433 0.14285718 0.28571433 0.3571429  0.28571433\n",
            " 0.5714286  0.3571429  0.42857146 0.42857146 0.21428575 0.35247016\n",
            " 0.42857146 0.14285718 0.28571433 0.21428575 0.42857146 0.28571433\n",
            " 0.5        0.5714286  0.64285713 0.14285718 0.5714286  0.28571433\n",
            " 0.42857146 0.71428573 0.28571433 0.21428575 0.71428573 0.3571429\n",
            " 0.71428573 0.21428575 0.28571433 0.28571433 0.42857146 0.42751145\n",
            " 0.21428575 0.40553632 0.5714286  0.5       ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-smshzds4bbk",
        "outputId": "dd0286f3-6591-449b-d046-6a124efa1887"
      },
      "source": [
        "from math import sqrt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "pearson_correlation = scipy.stats.pearsonr(sims,test_df['sts'])[0]\n",
        "spearman_correlation = scipy.stats.spearmanr(sims,test_df['sts'])[0]\n",
        "\n",
        "\n",
        "textstr = '\\nPearson Correlation=%.1f\\nSpearman Correlation=%.1f ' % ( pearson_correlation*100, spearman_correlation*100)\n",
        "print(textstr)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pearson Correlation=-16.9\n",
            "Spearman Correlation=-23.0 \n"
          ]
        }
      ]
    }
  ]
}